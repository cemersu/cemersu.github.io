---
title: "Building Simple Linear Regression from Scratch: No Libraries, Just Math"
date: 2025-11-26
categories:
  - Data Science
  - Projects
tags:
  - python
  - mathematics
  - linear regression
  - beginner
toc: true
toc_label: "Experiment Log"
toc_sticky: true
header:
  teaser: /assets/images/simplelr/linear-icon.png
---

**Welcome back to the lab. ðŸ§ª**

Today, I will show you how **Simple Linear Regression** models work under the hood and how you can build one from **scratch** without relying on "black box" libraries like Scikit-Learn.

## 1. Logic First: Understanding Correlation

Before we dive into equations, we must understand the behavior of data. Linear Regression is essentially an attempt to quantify the relationship between two variables.

Imagine you want to analyze the link between **Years of Experience** and **Salary**. Intuitively, we expect that as experience grows, the salary increases. In statistics, we call this a **Positive Correlation**. The variables move in the same direction.

Now, letâ€™s consider a different scenario: The relationship between **Parental Anger Levels** and **Allowed Gaming Hours**. As your mother gets angrier, your allowed gaming time likely drops to zero. Here, one value increases while the other decreases. This is a **Negative Correlation**.

Finally, if you were to plot the **Current Time of Day** against your **Bank Account Balance**, you would likely find no meaningful pattern. The time of day does not dictate your wealth. This is a case of **Zero Correlation**.

In Linear Regression, our goal is to find a mathematical line that best describes these positive or negative relationships so we can predict future outcomes.

## 2. The Math: The Method of Least Squares

We are looking for a line with the equation:

![Linear Regression Equation Drawing](/assets/images/simplelr/linear-icon.png)

* **y (Dependent Variable):** The outcome we want to predict (e.g., Salary).
* **x (Independent Variable):** The input data (e.g., Experience).
* **beta_1 (Slope):** The angle of the line.
* **beta_0 (Intercept):** Where the line starts on the y-axis.

To find the "perfect" line, we use a method called **Ordinary Least Squares (OLS)**. We want to minimize the errorâ€”specifically, the sum of the squared differences between our prediction and reality.

Using calculus, we derive two powerful formulas to find our coefficients (betas):

### 1. The Slope (beta_1)
This is calculated by dividing the **Covariance** of x and y (how they move together) by the **Variance** of x (how x moves on its own).

![Slope Formula Drawing](/assets/images/simplelr/linear-icon.png)

### 2. The Intercept (beta_0)
Once we have the slope, the line must pass through the average of our data points.

![Intercept Formula Drawing](/assets/images/simplelr/linear-icon.png)

# Implementation: Turning Math into Machine Learning

Now that we have derived the formulas for the slope ($\beta_1$) and the intercept ($\beta_0$), it is time to build the engine.

I have structured the code into two distinct parts:
1.  **The Engine (The Class):** Where the logic and "memory" live.
2.  **The Driver (Execution):** Where we feed data and test the model.

## Part 1: The Engine (The Class Logic)

We use Python's Object-Oriented Programming (OOP) to create a blueprint. This allows our model to have "state"â€”meaning it can learn something in one step and remember it for the next.

![Code: The Linear Regression Class Logic](/assets/images/simplelr/class-code.png)

### Breaking Down the Engine:
* **`__init__`**: This is the constructor. It initializes our slope and intercept to zero. The model starts with a "blank mind."
* **`fit(X, y)`**: This is the training phase. It takes the raw data, applies the **Least Squares** formulas we derived, and saves the results into `self.beta_0` and `self.beta_1`. It doesn't return a value; it simply updates its internal memory.
* **`predict(X_new)`**: This is the deployment phase. It takes new, unseen inputs and uses the *learned* coefficients to calculate predictions using the line equation $y = \beta_0 + \beta_1 x$.

---

## Part 2: The Driver (Training & Prediction)

A car engine is useless without fuel and a driver. In this section, we create some dummy data, instantiate our model, and take it for a test drive.

![Code: Training and Testing the Model](/assets/images/simplelr/train-pred-code.png)
![Code: Training and Testing the Model](/assets/images/simplelr/output-code.png)

### The Workflow:
1.  **Data Preparation:** We define simple lists for `X` (input) and `y` (target).
2.  **Instantiation:** `model = ErsuLinearRegression()` creates a new instance of our class.
3.  **Training:** We call `.fit(X, y)`. At this moment, the math happens, and the model "learns" the relationship.
4.  **Prediction:** We ask the model, "If X is 15, what is y?" and print the result to verify our logic.

---
